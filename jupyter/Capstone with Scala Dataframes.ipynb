{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{SQLContext, SaveMode}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now a UDFs, because Spark 1.4 does not have all of the functions we need\n",
    "This does a simple string concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val concat = udf((s1:String, s2:String) => s1 + s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes on the stores and receipts_by_store_date table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val stores_df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n",
    "      options(Map(\"keyspace\"-> \"retail\", \"table\" -> \"stores\")).\n",
    "      load()\n",
    "      \n",
    "val receipts_by_store_date_df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n",
    "      options(Map(\"keyspace\"-> \"retail\", \"table\" -> \"receipts_by_store_date\")).\n",
    "      load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the sales_by_state\n",
    "1. join receipts_by_store_date to store\n",
    "2. group by state\n",
    "3. sum by receipt_total\n",
    "4. do a select to add the dummy column, rename columns, compute the region and round the totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sales_by_state_df = receipts_by_store_date_df.\n",
    "      join(stores_df, stores_df(\"store_id\") === receipts_by_store_date_df(\"store_id\")).\n",
    "      groupBy(stores_df(\"state\")).\n",
    "      sum(\"receipt_total\").\n",
    "      select(lit(\"dummy\") alias \"dummy\", col(\"state\"), concat( lit(\"US-\"), col(\"state\")) alias \"region\", col(\"SUM(receipt_total)\") cast \"Decimal(10,2)\" alias (\"receipts_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+--------------+\n",
      "|dummy|state|region|receipts_total|\n",
      "+-----+-----+------+--------------+\n",
      "|dummy|   MS| US-MS|    5940679.25|\n",
      "|dummy|   MT| US-MT|    1916823.06|\n",
      "|dummy|   TN| US-TN|   10767153.95|\n",
      "|dummy|   NC| US-NC|   12888588.82|\n",
      "|dummy|   ND| US-ND|    2901100.69|\n",
      "|dummy|   NH| US-NH|    1957237.77|\n",
      "|dummy|   AL| US-AL|    5797108.70|\n",
      "|dummy|   NJ| US-NJ|   11580738.27|\n",
      "|dummy|   TX| US-TX|   25657021.59|\n",
      "|dummy|   NM| US-NM|    2988450.47|\n",
      "+-----+-----+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_by_state_df show 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save it to sales_by_state.  First truncate the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_by_state_df.write.\n",
    "      format(\"org.apache.spark.sql.cassandra\").\n",
    "      options(Map(\"keyspace\" -> \"retail\",\n",
    "                  \"table\" -> \"sales_by_state\")).\n",
    "      mode(SaveMode.Overwrite).\n",
    "      save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>dummy</th><th>receipts_total</th><th>state</th><th>region</th></tr><tr><td>dummy</td><td>29802833.74</td><td>FL</td><td>US-FL</td></tr><tr><td>dummy</td><td>25657021.59</td><td>TX</td><td>US-TX</td></tr><tr><td>dummy</td><td>20938106.75</td><td>PA</td><td>US-PA</td></tr><tr><td>dummy</td><td>19994663.81</td><td>CA</td><td>US-CA</td></tr><tr><td>dummy</td><td>16736304.37</td><td>NY</td><td>US-NY</td></tr></table>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%cql select * from retail.sales_by_state limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark-DSE Cluster (Scala 2.10.4)",
   "language": "scala",
   "name": "spark-dse-cluster"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
